{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"\nimport argparse\nimport re\n\nfrom pyspark.sql import SparkSession\n\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.regression import RandomForestRegressor, RandomForestRegressionModel\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\n\ndef process(params):\n\n    #\n    # Initializing Spark session\n    #\n    sparkSession = (SparkSession.builder\n      .appName(\"AllstateClaimsSeverityRandomForestRegressor\")\n      .getOrCreate())\n\n    #****************************\n    print(\"Loading input data\")\n    #****************************\n\n    if (params.trainInput.startswith(\"s3://\")):\n        sparkSession.conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n        sparkSession.conf.set(\"spark.hadoop.fs.s3a.access.key\", params.s3AccessKey)\n        sparkSession.conf.set(\"spark.hadoop.fs.s3a.secret.key\", params.s3SecretKey)\n\n    #*************************************************\n    print(\"Reading data from train.csv file\")\n    #*************************************************\n\n    trainInput = (sparkSession.read\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"true\")\n      .csv(params.trainInput)\n      .cache())\n\n    testInput = (sparkSession.read\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"true\")\n      .csv(params.testInput)\n      .cache())\n\n    #*****************************************\n    print(\"Preparing data for training model\")\n    #*****************************************\n\n    data = (trainInput.withColumnRenamed(\"loss\", \"label\")\n      .sample(False, params.trainSample))\n\n    [trainingData, validationData] = data.randomSplit([0.7, 0.3])\n\n    trainingData.cache()\n    validationData.cache()\n\n    testData = testInput.sample(False, params.testSample).cache()\n\n   \n    print(\"Building Machine Learning pipeline\")\n \n\n    #StringIndexer for categorical columns (OneHotEncoder should be evaluated as well)\n    isCateg     = lambda c: c.startswith(\"cat\")\n    categNewCol = lambda c: \"idx_{0}\".format(c) if (isCateg(c)) else c\n\n    stringIndexerStages = map(lambda c: StringIndexer(inputCol=c, outputCol=categNewCol(c))\n        .fit(trainInput.select(c).union(testInput.select(c))), filter(isCateg, trainingData.columns))\n\n    #Function to remove categorical columns with too many categories\n    removeTooManyCategs = lambda c: not re.match(r\"cat(109$|110$|112$|113$|116$)\", c)\n\n    #Function to select only feature columns (omit id and label)\n    onlyFeatureCols = lambda c: not re.match(r\"id|label\", c)\n\n    #Definitive set of feature columns\n    featureCols = map(categNewCol, \n                      filter(onlyFeatureCols, \n                             filter(removeTooManyCategs, \n                                    trainingData.columns)))\n\n    #VectorAssembler for training features\n    assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n\n    #Estimator algorithm\n    algo = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n    \n    stages = stringIndexerStages\n    stages.append(assembler)\n    stages.append(algo)\n\n    #Building the Pipeline for transformations and predictor\n    pipeline = Pipeline(stages=stages)\n\n\n    #*********************************************************\n    print(\"Preparing K-fold Cross Validation and Grid Search\")\n    #*********************************************************\n\n    paramGrid = (ParamGridBuilder()\n      .addGrid(algo.numTrees, params.algoNumTrees)\n      .addGrid(algo.maxDepth, params.algoMaxDepth)\n      .addGrid(algo.maxBins, params.algoMaxBins)\n      .build())\n      \n    cv = CrossValidator(estimator=pipeline,\n                        evaluator=RegressionEvaluator(),\n                        estimatorParamMaps=paramGrid,\n                        numFolds=params.numFolds)\n\n\n    #**********************************************************\n    print(\"Training model with RandomForest algorithm\")\n    #**********************************************************\n\n    cvModel = cv.fit(trainingData)\n\n    print(\"Evaluating model on train and test data and calculating RMSE\")\n   \n    trainPredictionsAndLabels = cvModel.transform(trainingData).select(\"label\", \"prediction\").rdd\n\n    validPredictionsAndLabels = cvModel.transform(validationData).select(\"label\", \"prediction\").rdd\n\n    trainRegressionMetrics = RegressionMetrics(trainPredictionsAndLabels)\n    validRegressionMetrics = RegressionMetrics(validPredictionsAndLabels)\n\n    bestModel = cvModel.bestModel\n    featureImportances = bestModel.stages[-1].featureImportances.toArray()\n\n    output = (\"\\n====\\n\" +\n      \"Param trainSample: {0}\\n\".format(params.trainSample) +\n      \"Param testSample: {0}\\n\".format(params.testSample) +\n      \"TrainingData count: {0}\\n\".format(trainingData.count()) +\n      \"ValidationData count: {0}\\n\".format(validationData.count()) +\n      \"TestData count: {0}\\n\".format(testData.count()) +\n      \"=====================================================================\\n\" +\n      \"Param algoNumTrees = {0}\\n\".format(\",\".join(params.algoNumTrees)) +\n      \"Param algoMaxDepth = {0}\\n\".format(\",\".join(params.algoMaxDepth)) +\n      \"Param algoMaxBins = {0}\\n\".format(\",\".join(params.algoMaxBins)) +\n      \"Param numFolds = {0}\\n\".format(params.numFolds) +\n      \"=====================================================================\\n\" +\n      \"Training data MSE = {0}\\n\".format(trainRegressionMetrics.meanSquaredError) +\n      \"Training data RMSE = {0}\\n\".format(trainRegressionMetrics.rootMeanSquaredError) +\n      \"Training data R-squared = {0}\\n\".format(trainRegressionMetrics.r2) +\n      \"Training data MAE = {0}\\n\".format(trainRegressionMetrics.meanAbsoluteError) +\n      \"Training data Explained variance = {0}\\n\".format(trainRegressionMetrics.explainedVariance) +\n      \"=====================================================================\\n\" +\n      \"Validation data MSE = {0}\\n\".format(validRegressionMetrics.meanSquaredError) +\n      \"Validation data RMSE = {0}\\n\".format(validRegressionMetrics.rootMeanSquaredError) +\n      \"Validation data R-squared = {0}\\n\".format(validRegressionMetrics.r2) +\n      \"Validation data MAE = {0}\\n\".format(validRegressionMetrics.meanAbsoluteError) +\n      \"Validation data Explained variance = {0}\\n\".format(validRegressionMetrics.explainedVariance) +\n      \"=====================================================================\\n\" +\n      # \"CV params explained: ${cvModel.explainParams()}\\n\" +\n      # \"RandomForest params explained: ${bestModel.stages[-1].explainParams()}\\n\" +\n      \"RandomForest features importances:\\n {0}\\n\".format(\"\\n\".join(map(lambda z: \"{0} = {1}\".format(str(z[0]),str(z[1])), zip(featureCols, featureImportances)))) +\n      \"=====================================================================\\n\")\n\n    print(output)\n\n\n    print(\"Run prediction over test dataset\")\n\n\n    #Predicts and saves file ready for Kaggle!\n    if params.outputFile:\n        (cvModel.transform(testData)\n          .select(\"id\", \"prediction\")\n          .withColumnRenamed(\"prediction\", \"loss\")\n          .coalesce(1)\n          .write.format(\"csv\")\n          .option(\"header\", \"true\")\n          .save(params.outputFile))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--s3AccessKey\", help=\"The access key for S3\", required=True)\n    parser.add_argument(\"--s3SecretKey\", help=\"The secret key for S3\", required=True)\n    parser.add_argument(\"--trainInput\",  help=\"Path to file/directory for training data\", required=True)\n    parser.add_argument(\"--testInput\",   help=\"Path to file/directory for test data\", required=True)\n    parser.add_argument(\"--outputFile\",  help=\"Path to output file\")\n    parser.add_argument(\"--algoNumTrees\", nargs='+', type=int, help=\"One or more options for number of trees for RandomForest model. Default: 3\", default=[3])\n    parser.add_argument(\"--algoMaxDepth\", nargs='+', type=int, help=\"One or more values for depth limit. Default: 4\", default=[4])\n    parser.add_argument(\"--algoMaxBins\",  nargs='+', type=int, help=\"One or more values for max bins for RandomForest model. Default: 32\", default=[32])\n    parser.add_argument(\"--numFolds\",    type=int,   help=\"Number of folds for K-fold Cross Validation. Default: 10\", default=10)\n    parser.add_argument(\"--trainSample\", type=float, help=\"Sample fraction from 0.0 to 1.0 for train data\", default=1.0)\n    parser.add_argument(\"--testSample\",  type=float, help=\"Sample fraction from 0.0 to 1.0 for test data\", default=1.0)\n\n    params = parser.parse_args()\n\n    process(params)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}